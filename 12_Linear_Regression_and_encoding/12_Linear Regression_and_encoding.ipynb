{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648d7847",
   "metadata": {},
   "source": [
    "# Linear regression and introduction to dummy variables and one hot encoding\n",
    "1) Understand the Problem or situation or create a hypothesis. \n",
    "2) Obtain the data or extract the data \n",
    "3) Data cleaning or Data prep\n",
    "4) Explore the data\n",
    "5) Feature Selection\n",
    "6) Algorithm Selection\n",
    "7) Tuning the Model\n",
    "8) Evaluating the Model\n",
    "9) Presentation\n",
    "\n",
    "scikit-learn's 6-step modeling pattern Takes place in Step 5,6,7,8\n",
    "\n",
    "Step 1: Create a feature matrix and response vector\n",
    "\n",
    "Step 2: Decide on the estimator you want to to use and import that class\n",
    "\n",
    "Step 3: \"Instantiate\" the \"estimator\"\n",
    "\n",
    "Step 4: Fit the model with data (aka \"model training\")\n",
    "\n",
    "Step 5: Use the model to predict the response for a new observation\n",
    "\n",
    "Step 6: Evaluate the error or accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ddb82",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "    Simple to explain\n",
    "    Highly interpretable\n",
    "    Model training and prediction are fast\n",
    "    No tuning is required (excluding regularization)\n",
    "    Features don't need scaling\n",
    "    Can perform well with a small number of observations\n",
    "    Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "    Presumes a linear relationship between the features and the response\n",
    "    Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "    Can't automatically learn feature interactions\n",
    "\n",
    "In simple terms it is the relationship between two variables\n",
    "Another way to think of it is the value of the dependent variable at certain value of the independent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e125977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minutes = [100,150,200,300,350,400,450]\n",
    "Grade = [3,4,5,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10141d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x = Minutes, y = Grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83475c9",
   "metadata": {},
   "source": [
    "We dont have this data point but we want to inform students that if they studied more than 200 minutes like maybe 250 minutes they could hypothetically get a grade of???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90171773",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minutes = [100,100,150,98,103,350,195,205,200,300,310,350, 400,450, 435]\n",
    "Grade = [3,3.3,4,3.4,3,8,5,4.9,5,7,6.5, 8.6, 9,10, 9.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a295467",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = Minutes, y = Grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0437dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minutes = [100,150,200,300,350,400,450,150]\n",
    "Grade = [3,4,5,7,8,9,10,9.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = Minutes, y = Grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b12454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('C://Users//Matth//OneDrive//Desktop/DATA//Student_Grade.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb72087",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x = 'Study Time In Minutes', y = 'Credit Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x =\"Study Time In Minutes\", y =\"Credit Grade\", data = df, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db5a38",
   "metadata": {},
   "source": [
    "Steps for modeling\n",
    "- Step 1: Understanding the Problem.\n",
    "- Step 2: Data Extraction.\n",
    "- Step 3: Data Cleaning.\n",
    "- Step 4: Exploratory Data Analysis.\n",
    "- Step 5: Feature Selection. <--- you are here\n",
    "- Step 6: Incorporating Machine Learning Algorithms. <-- and here\n",
    "- Step 7: Testing the Models. <-- and here\n",
    "- Step 8: Deploying the Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fa62f",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the Problem.\n",
    "If you study more then your grades will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166823f8",
   "metadata": {},
   "source": [
    "### Step 2: Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842daf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lets first start by importing All the Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as linreg\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# Data Extraction\n",
    "df = pd.read_csv('C://Users//Matth//OneDrive//Desktop/DATA//Student_Grade.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c441807",
   "metadata": {},
   "source": [
    "### Step 3: Data Cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9faf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3: Data Cleaning.\n",
    "'''Data cleansing can be an extensive and iterative process, \n",
    "this can involve removing duplicates, removing or working with Nans and zeros. \n",
    "renaming fields, accounting for outliers and more. For now we will keep it simple with \n",
    "A)Selecting attributes we are interested in(some tables will have lots of attributes and we may want to \n",
    "isolate to a few)\n",
    "B) Renaming fields for ease of use or readability\n",
    "'''\n",
    "# Select the two attributes intrested in\n",
    "df2 = df[['Study Time In Minutes', 'Credit Grade']]\n",
    "# Rename attributes if needed\n",
    "df2.rename(columns={'Study Time In Minutes':'Study Min', 'Credit Grade':'Grade'}, inplace=True)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf70ddb",
   "metadata": {},
   "source": [
    "### Step 4 EDA\n",
    "Just like with most of these steps there can be a lot of visualizations and work done in the EDA section. For just getting the flow down we will limit it to one visualization.\n",
    "EDA can often times lead to discoveries and more data cleansing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x =\"Study Min\", y =\"Grade\", data = df2, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874982d",
   "metadata": {},
   "source": [
    "### Step 5 Feature Selection\n",
    "In this example there is only one feature to select in future examples there will be more features. \n",
    "If you have 10 attributes you may want to keep them in your data set but only test or model 3 of them as \n",
    "your feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df2[['Study Min']]\n",
    "target = df2[['Grade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a21735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('R-Squared (R\\u00b2): {:.2f}%'.format(r2 * 100))\n",
    "print('Mean Squared Error (MSE): {:.2f}'.format(mse))\n",
    "print('Mean Absolute Error:{:.2f}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e16b70",
   "metadata": {},
   "source": [
    "- R-Square/Adjusted R square\n",
    "R-squared, also known as the coefficient of determination, is a widely used metric to assess the goodness-of-fit of a linear regression model. It represents the proportion of the variance in the dependent variable that is explained by the independent variables.\n",
    "- Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable (Y-axis).\n",
    "- Mean Absolute Error MAE\n",
    "To put it in short, if there are many outliers then you may consider using Mean Absolute Error (also called the Average Absolute Deviation). RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87418048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff69438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "plt.scatter(X_train, y_train, color = 'lightgreen')\n",
    "plt.plot(X_train, y_pred_train, color = 'red')\n",
    "plt.legend(['fix', 'fix' ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set and predictions\n",
    "plt.scatter(X_test, y_test, color ='lightblue')\n",
    "plt.plot(X_train, y_pred_train, color ='firebrick')\n",
    "plt.legend(['Fix','Fix'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01769254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients# the value of y in x = 0 \n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "attachments": {
    "image-3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b3f45b27",
   "metadata": {},
   "source": [
    "## Dummy Variables and One-hot encoding\n",
    "These are two different methods that have the same outcomes but can provide slight difference in benefits. \n",
    "![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80931aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#create DataFrame\n",
    "df1 = pd.DataFrame({'area': ['rural', 'rural', 'urban', 'urban', 'suburban', 'suburban', 'rural', 'urban'],\n",
    "                   'rating': [25, 12, 15, 14, 19, 23, 25, 29]})\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#create DataFrame\n",
    "df2 = pd.DataFrame({'area': ['rural', 'rural', 'urban', 'urban', 'suburban', 'suburban', 'rural', 'urban'],\n",
    "                   'rating': [25, 12, 15, 14, 19, 23, 25, 29]})\n",
    "#view DataFrame\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'team' column \n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df2[['area']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df2.join(encoder_df)\n",
    "\n",
    "#view final df\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3cbe8",
   "metadata": {},
   "source": [
    "### Classic BIKESHARE example (putting it all together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73956b",
   "metadata": {},
   "source": [
    "# What is the bikeshare dataset\n",
    "We'll be working with a dataset from Capital Bikeshare that was used in a Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7672c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data set and setting the datetime as the index\n",
    "df = pd.read_csv('C://Users//Matth//OneDrive//Desktop/DATA//bikeshare.csv', index_col='datetime', parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dd500",
   "metadata": {},
   "source": [
    "Questions:\n",
    "    How many Features are there\n",
    "    What does each observation represent?\n",
    "    What is the response variable?\n",
    "    Where does your curiosity guide you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbfd7d",
   "metadata": {},
   "source": [
    "Are there blanks?\n",
    "Are there duplicates?\n",
    "Are there fields that could be dropped or renamed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c013863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"count\" is a method, so it's best to name that column something else\n",
    "df.rename(columns={'count':'total'}, inplace=True)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578af32",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas scatter plot\n",
    "df.plot(kind='scatter', x='temp', y='total', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15056cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatter plot with regression line\n",
    "sns.lmplot(x='temp', y='total', data=df, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff5ff6",
   "metadata": {},
   "source": [
    "\n",
    "Building a simple linear regression model (one feature)\n",
    "\n",
    "For our first task we decide to create a model which will predict the number of rentals based on the temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['temp']\n",
    "X = df[feature_cols]\n",
    "y = df.total\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef66a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2db371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created our X (feature matrix) and Y (response vector so now lets:\n",
    "# import our chosen estimator, instantiate it into a variable, fit the model with the X and y\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12227b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718f459",
   "metadata": {},
   "source": [
    "Visualizing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd868a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore more features\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatter plots in Seaborn\n",
    "sns.pairplot(df, x_vars=feature_cols, y_vars='total', kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a19d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatter plots in matplotlib\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    df.plot(kind='scatter', x=feature, y='total', ax=axs[index], figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08967aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-tabulation of season and month\n",
    "pd.crosstab(df.season, df.index.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce910b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot of rentals, grouped by season\n",
    "df.boxplot(column='total', by='season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d92f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot of rentals\n",
    "df.total.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45da81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing overall growth and the winter months happen to come after the spring months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd58931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix (ranges from 1 to -1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058dcdf",
   "metadata": {},
   "source": [
    "What relationships do you notice?\n",
    "Adding more features to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ed556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of features\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']\n",
    "\n",
    "# create X and y\n",
    "X = df[feature_cols]\n",
    "y = df.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87df02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "zip(feature_cols, linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f0c60",
   "metadata": {},
   "source": [
    "\n",
    "Feature selection\n",
    "\n",
    "How do we choose which features to include in the model? We're going to use train/test split (and eventually cross-validation).\n",
    "\n",
    "Why not use of p-values or R-squared for feature selection?\n",
    "\n",
    "    Linear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train/test split relies on fewer assumptions.\n",
    "    Features that are unrelated to the response can still have significant p-values.\n",
    "    Adding features to your model that are unrelated to the response will always increase the R-squared value, and adjusted R-squared does not sufficiently account for this.\n",
    "    p-values and R-squared are proxies for our goal of generalization, whereas train/test split and cross-validation attempt to directly estimate how well the model will generalize to out-of-sample data.\n",
    "\n",
    "More generally:\n",
    "\n",
    "    There are different methodologies that can be used for solving any given data science problem, and this course follows a machine learning methodology.\n",
    "    This course focuses on general purpose approaches that can be applied to any model, rather than model-specific approaches.\n",
    "\n",
    "Evaluation metrics for regression problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as accuracy, are not useful for regression problems. We need evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n",
    "    Mean Squared Error (MSE) is the mean of the squared errors:\n",
    "        Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c854a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed063c6",
   "metadata": {},
   "source": [
    "Comparing these metrics:\n",
    "\n",
    "    MAE is the easiest to understand, because it's the average error.\n",
    "    MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "    RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are loss functions, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punish larger errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# new set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "# MAE is the same as before\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "\n",
    "# MSE and RMSE are larger than before\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f22b2",
   "metadata": {},
   "source": [
    "Comparing models with train/test split and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define a function that accepts a list of features and returns testing RMSE\n",
    "def train_test_rmse(feature_cols):\n",
    "    X = bikes[feature_cols]\n",
    "    y = bikes.total\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(X, y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different sets of features\n",
    "print(train_test_rmse(['temp', 'season', 'weather', 'humidity']))\n",
    "print(train_test_rmse(['temp', 'season', 'weather']))\n",
    "print(train_test_rmse(['temp', 'season', 'humidity']))\n",
    "print(train_test_rmse(['temp', 'humidity']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2957625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using these as features is not allowed!\n",
    "print(train_test_rmse(['casual', 'registered']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e948616",
   "metadata": {},
   "source": [
    "Comparing testing RMSE with null RMSE\n",
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f18ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "# create a NumPy array with the same shape as y_test\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# fill the array with the mean value of y_test\n",
    "y_null.fill(y_test.mean())\n",
    "y_null\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute null RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948a35c",
   "metadata": {},
   "source": [
    "Handling categorical features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "    Ordered categories: transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "    Unordered categories: use dummy encoding (0/1)\n",
    "\n",
    "What are the categorical features in our dataset?\n",
    "\n",
    "    Ordered categories: weather (already encoded with sensible numeric values)\n",
    "    Unordered categories: season (needs dummy encoding), holiday (already dummy encoded), workingday (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an ordered relationship. Instead, we create multiple dummy variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "season_dummies = pd.get_dummies(df.season, prefix='season')\n",
    "\n",
    "# print 5 random rows\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437d304",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "However, we actually only need three dummy variables (not four), and thus we'll drop the first dummy variable.\n",
    "\n",
    "Why? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the baseline level:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c353974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first column\n",
    "season_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# print 5 random rows\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a33250",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In general, if you have a categorical feature with k possible values, you create k-1 dummy variables.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for holiday, not two dummy variables (holiday_yes and holiday_no).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "bikes = pd.concat([df, season_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaa958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 random rows\n",
    "bikes.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include dummy variables for season in the model\n",
    "# include dummy variables for season in the model\n",
    "feature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "zip(feature_cols, linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f21f31",
   "metadata": {},
   "source": [
    "How do we interpret the season coefficients? They are measured against the baseline (spring):\n",
    "\n",
    "    Holding all other features fixed, summer is associated with a rental decrease of 3.39 bikes compared to the spring.\n",
    "    Holding all other features fixed, fall is associated with a rental decrease of 41.7 bikes compared to the spring.\n",
    "    Holding all other features fixed, winter is associated with a rental increase of 64.4 bikes compared to the spring.\n",
    "\n",
    "Would it matter if we changed which season was defined as the baseline?\n",
    "\n",
    "    No, it would simply change our interpretation of the coefficients.\n",
    "\n",
    "Important: Dummy encoding is relevant for all machine learning models, not just linear regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare original season variable with dummy variables\n",
    "print(train_test_rmse(['temp', 'season', 'humidity']))\n",
    "print(train_test_rmse(['temp', 'season_2', 'season_3', 'season_4', 'humidity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90975ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Feature engineering\n",
    "\n",
    "See if you can create the following features:\n",
    "\n",
    "    hour: as a single numeric feature (0 through 23)\n",
    "    hour: as a categorical feature (use 23 dummy variables)\n",
    "    daytime: as a single categorical feature (daytime=1 from 7am to 8pm, and daytime=0 otherwise)\n",
    "\n",
    "Then, try using each of the three features (on its own) with train_test_rmse to see which one performs the best!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour as a numeric feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daytime as a categorical feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00070d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour as a categorical feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584dfbb",
   "metadata": {},
   "source": [
    "Comparing linear regression with other models\n",
    " \n",
    "Advantages of linear regression:\n",
    " \n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    " \n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e18cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5f989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
