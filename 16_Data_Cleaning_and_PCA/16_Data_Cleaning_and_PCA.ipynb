{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Cleaning with Python\n",
    "## Review of basic Python\n",
    "## Importing Data \n",
    "## Basic Exploration\n",
    "## Intro to Cleaning Data\n",
    "## Missing Data Techniques\n",
    "## Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# LOAD A FILE \n",
    "df = pd.read_csv('C://Users//msmorris//Documents//PythonScripts//DATA//sample_galaxy_actuals(pre-allocated).csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df) # DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) # print the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # print the last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index # \"the index\" (aka \"the labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # column names (which is \"an index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values # underlying numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.productid # select one column using the DataFrame attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['productid'] # select one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['productid']) # check type on one column or series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.channelid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['channelid'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['localamount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display length\n",
    "df['companyid'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data type\n",
    "df['companyid'] = df['companyid'].astype(str)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length after changing data type plus unique counts\n",
    "df['companyid'].astype(str).str.len().unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['companyid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show companyid 41\n",
    "filt1 = df.companyid == '41' \n",
    "# use as a filter for rest of dataframe\n",
    "df[filt1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same results but in 1 step\n",
    "df[df.companyid == '41']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one column from the filtered results\n",
    "df[df.companyid == '41'].costcenterid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts of resulting Series \n",
    "df[df.companyid == '41'].costcenterid.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean filtering with multiple conditions\n",
    "df[(df.companyid == '41') & (df.costcenterid > 5000)] # ampersand for AND condition\n",
    "df[(df.companyid == '41') | (df.costcenterid > 5000)] # pipe for OR condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['localamount'].order() # sort a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['companyid', 'costcenterid']) # sort a DataFrame by a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='accountid', ascending=False) # use descending order instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put NA, NAN NAS first\n",
    "df.sort_values(by='revgpid', ascending=False, na_position='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object']) # describe all object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all') # describe all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.localamount.describe() # describe a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.localamount.mean() # only calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.companid.value_counts() # most useful for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['companyid','costcenterid','accountid','locationid', 'productid']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Unique and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revgpid'].unique() #display unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countd = df['productid'].value_counts() #counting the unique values\n",
    "print(df_countd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.duplicated()] # display duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['localamount'].isnull().sum() # sums null values field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify missing values\n",
    "df.loc[df['localamount'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows\n",
    "df2 = df.dropna()\n",
    "#identify missing values\n",
    "df2.loc[df['localamount'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing rows\n",
    "df3 = df['localamount'] = df['localamount'].fillna(df['localamount'].median())\n",
    "df3.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing column values\n",
    "df['locationzone'] = [i if i[1] in '0123456789' else i[0]+'5' for i in df['locationzone']]\n",
    "print(df['locationzone'])\n",
    "\n",
    "\n",
    "#creating a separate dataset\n",
    "df2 = df.loc[df['locationdistrict'].isin(['H','J','N'])][['locationzone','locationdistrict','date_appraised','medv']]\n",
    "\n",
    "#ensuring column names\n",
    "df['productid'].unique()\n",
    "print(df['productid'])\n",
    "\n",
    "#creating a new column\n",
    "df['month_appraised'] = df['date_appraised'].apply(lambda x: str(x)[:7])\n",
    "df['month_appraised'].unique()\n",
    "\n",
    "#ensuring data format\n",
    "df['date_appraised'] = pd.to_datetime(df['date_appraised'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#isolating values\n",
    "data = df.loc[df['locationid'].astype(str).str.len() != 3]\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ STATS #################\n",
    "\n",
    "\n",
    "\n",
    "#display mean value per companyid\n",
    "df2 = df.loc[df['constantamount'] >= 0]\n",
    "tbl = df2.groupby('companyid')['constantamount'].mean().round(1)\n",
    "print(tbl)\n",
    "\n",
    "#identify outlier\n",
    "medv_z = (df['constantamount'] - df['constantamount'].mean()) /df['constantamount'].std()\n",
    "data = df.loc[(medv_z > 3) | (medv_z < -3)]\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "#create histograms\n",
    "df[ ['constantamount'] ].hist()\n",
    "\n",
    "#display barchart\n",
    "df.groupby('date_appraised').size().plot.bar()\n",
    "\n",
    "#plotting a barchart\n",
    "df.groupby('locationdistrict').size().plot.bar()\n",
    "\n",
    "#display max value\n",
    "df.groupby('locationzone')['nox'].max().plot.bar()\n",
    "plt.savefig('uc_pyplot.jpg')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#plotting a boxplot\n",
    "df.boxplot(['medv','crim'])\n",
    "plt.savefig('uc_pyplot.jpg')\n",
    "plt.close()\n",
    "\n",
    "#plotting boxplot\n",
    "sns.boxplot(x='Species',y='Sepal.Width', data=df)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### IN DEV ###############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#displaying subset\n",
    "data_numeric = data[['crim','zn','chas','nox','age','dis','tax','ptratio','lstat','medv']]\n",
    "print(data_numeric)\n",
    "\n",
    "#display PCA names\n",
    "data_numeric = data[ ['crim','zn','chas','nox','age','dis','tax','ptratio','lstat','medv'] ]\n",
    "pcs_names = []\n",
    "for i, col in enumerate(data_numeric.columns):\n",
    "    pcs_names.append('PC' + str(i+1))\n",
    "print(pcs_names)\n",
    "\n",
    "#normalizing dataset\n",
    "data_normed = (data_numeric - data_numeric.mean()) / data_numeric.std()\n",
    "print(data_normed)\n",
    "\n",
    "#specifying number of components\n",
    "pca = PCA(n_components=data_normed.shape[1])\n",
    "pca.fit(data_normed)\n",
    "data_pca = pd.DataFrame(pca.transform(data_normed),columns = pcs_names)\n",
    "print(data_pca)\n",
    "\n",
    "#creating a scree plot\n",
    "sns.lineplot(y = pca.explained_variance_ratio_, x = range(1,11))\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xlabel('Principal component')\n",
    "\n",
    "#selecting the fewest components\n",
    "for pc, var in zip(pcs_names,np.cumsum(pca.explained_variance_ratio_)):\n",
    "   print(pc,var)\n",
    "   \n",
    "#creating a rotation\n",
    "rotation = pd.DataFrame(pca.components_.T, columns = pcs_names, index = data_numeric.columns)\n",
    "print(rotation)\n",
    "\n",
    "#creating components\n",
    "data_reduced = data_pca.iloc[:,0:6]\n",
    "print(data_reduced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "'''\n",
    "EXERCISE ONE\n",
    "'''\n",
    "\n",
    "# read drinks.csv into a DataFrame called 'drinks'\n",
    "drinks = pd.read_table('drinks.csv', sep=',')\n",
    "drinks = pd.read_csv('drinks.csv') # second solution, assumes separator is comma\n",
    "\n",
    "# print the head and the tail\n",
    "drinks.head()\n",
    "drinks.tail()\n",
    "\n",
    "# examine the default index, data types, and shape\n",
    "drinks.index\n",
    "drinks.dtypes\n",
    "drinks.shape\n",
    "\n",
    "# print the 'beer_servings' Series\n",
    "drinks['beer_servings']\n",
    "drinks.beer_servings\n",
    "\n",
    "# calculate the mean 'beer_servings' for the entire dataset\n",
    "drinks.describe() # summarize all numeric columns\n",
    "drinks.beer_servings.describe() # summarize only the 'beer_servings' Series\n",
    "drinks.beer_servings.mean() # only calculate the mean\n",
    "\n",
    "# count the number of occurrences of each 'continent' value and see if it looks correct\n",
    "drinks.continent.value_counts()\n",
    "\n",
    "# BONUS: display only the number of rows of the 'users' DataFrame\n",
    "users.shape[0] # display only the number of rows of the 'users' DataFrame\n",
    "\n",
    "\n",
    "# BONUS: display the 3 most frequent occupations in 'users'\n",
    "users.occupation.value_counts().head(3) #display the 3 most frequent occupations in 'users'\n",
    "users.occupation.value_counts()[:3]\n",
    "\n",
    "\n",
    "####### creating headers when they are missing ################################\n",
    "# BONUS: create the 'users' DataFrame from the u.user_original file (which lacks a header row)\n",
    "# Hint: read the pandas.read_table documentation\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_table('u.user_original', sep='|', header=None, names=user_cols, index_col='user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OVERVIEW / EXPLORING USING PANDAS\n",
    "### Review arrays Numpy\n",
    "### basic exploration\n",
    "### filtering\n",
    "### basic summaries\n",
    "### identify unique and duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy for numeric and pandas for dataframes or mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0,2])\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign filename: file\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import data as floats and skip the first row: data_float\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])\n",
    "\n",
    "# Plot a scatterplot of the data\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)\n",
    "data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = pd.read_csv(file,nrows=5,header=None)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = data.values\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t', comment=\"#\", na_values=[\"Nothing\"])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xl\n",
    "xl = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xl.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xl.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"select * from employee where EmployeeId >= 6 order by BirthDate\",engine) \n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.DataFrame(pd.read_sql_query(\"select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds < 250000\",engine))\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing[\"duration_trim\"].astype(int)\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration\n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print((ride_sharing['duration_time']).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Out of Range data\n",
    "#### drop data\n",
    "#### Custom min and max\n",
    "#### treat as missing and impute a value or calc ie median\n",
    "#### set a custom value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing[\"tire_sizes\"] > 27, \"tire_sizes\"] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype(\"category\")\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication errors\n",
    "#### double entry\n",
    "#### production  or design bugs\n",
    "#### Join or Merge from consolidated data inaccuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(\"ride_id\", keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_sharing.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'])\n",
    "print(airlines['dest_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges,\n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n",
    "            'Thursday': 'weekday', 'Friday': 'weekday',\n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\", \"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\", \"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\", \"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "#banking.loc[banking['acct_cur']=='euro', 'acct_amount'] = banking.loc[banking['acct_cur']=='euro', 'acct_amount'] * 1.1\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce')\n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.\n",
    "    Missing at Random: There is a systematic relationship between a column's missing values and other observed values.\n",
    "    Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount']*5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit = len(restaurants))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1]>=80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:\n",
    "    # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "    matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "    # Iterate through the list of matches\n",
    "    for match in matches:\n",
    "        # Check whether the similarity score is greater than or equal to 80\n",
    "        if match[1] >= 80:\n",
    "            # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "            restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result\n",
    "restaurants['cuisine_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types -\n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NUll value operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the sum of two None's\n",
    "  print(\"Add operation output of 'None': \", None + None)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'None' does not support Arithmetic Operations!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the sum of two np.nan's\n",
    "  print(\"Add operation output of 'np.nan': \", np.nan + np.nan)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'np.nan' does not support Arithmetic Operations!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the output of logical OR of two None's\n",
    "  print(\"OR operation output of 'None': \", None or None)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'None' does not support Logical Operations!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the output of logical OR of two np.nan's\n",
    "  print(\"OR operation output of 'np.nan': \", np.nan or np.nan)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'np.nan' does not support Logical Operations!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the comparison of two 'None's\n",
    "  print(\"'None' comparison output: \", None == None)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'None' does not support this operation!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Print the comparison of two 'np.nan's\n",
    "  print(\"'np.nan' comparison output: \", np.nan == np.nan)\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"'np.nan' does not support this operation!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Check if 'None' is 'NaN'\n",
    "  print(\"Is 'None' same as nan? \", np.isnan(None))\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"Function 'np.isnan()' does not support this Type!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Check if 'np.nan' is 'NaN'\n",
    "  print(\"Is 'np.nan' same as nan? \", np.isnan(np.nan))\n",
    "\n",
    "except TypeError:\n",
    "  # Print if error\n",
    "  print(\"Function 'np.isnan()' does not support this Type!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset 'college.csv'\n",
    "college = pd.read_csv('college.csv')\n",
    "print(college.head())\n",
    "\n",
    "# Print the info of college\n",
    "print(college.info())\n",
    "\n",
    "# Store unique values of 'csat' column to 'csat_unique'\n",
    "csat_unique = college.csat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset 'college.csv'\n",
    "college = pd.read_csv('college.csv')\n",
    "print(college.head())\n",
    "\n",
    "# Print the info of college\n",
    "print(college.info())\n",
    "\n",
    "# Store unique values of 'csat' column to 'csat_unique'\n",
    "csat_unique = college.csat.unique()\n",
    "\n",
    "# Print the sorted values of csat_unique\n",
    "print(np.sort(csat_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset 'college.csv' with na_values set to '.'\n",
    "college = pd.read_csv('college.csv', na_values='.')\n",
    "print(college.head())\n",
    "\n",
    "# Print the info of college\n",
    "print(college.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the description of the data\n",
    "print(diabetes.describe())\n",
    "\n",
    "# Store all rows of column 'BMI' which are equal to 0 \n",
    "zero_bmi = diabetes.BMI[diabetes.BMI == 0]\n",
    "print(zero_bmi)\n",
    "\n",
    "# Set the 0 values of column 'BMI' to np.nan\n",
    "diabetes.BMI[diabetes.BMI == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the description of the data\n",
    "print(diabetes.describe())\n",
    "\n",
    "# Store all rows of column 'BMI' which are equal to 0 \n",
    "zero_bmi = diabetes.BMI[diabetes.BMI == 0]\n",
    "print(zero_bmi)\n",
    "\n",
    "# Set the 0 values of column 'BMI' to np.nan\n",
    "diabetes.BMI[diabetes.BMI == 0] = np.nan\n",
    "\n",
    "# Print the 'NaN' values in the column BMI\n",
    "print(diabetes.BMI[np.isnan(diabetes.BMI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the airquality dataset\n",
    "airquality = pd.read_csv('air-quality.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Create a nullity DataFrame airquality_nullity\n",
    "airquality_nullity = airquality.isnull()\n",
    "print(airquality_nullity.head())\n",
    "\n",
    "# Calculate total of missing values\n",
    "missing_values_sum = airquality_nullity.sum()\n",
    "print('Total Missing Values:\\n', missing_values_sum)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_values_percent = airquality_nullity.mean() * 100\n",
    "print('Percentage of Missing Values:\\n', missing_values_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import missingno as msno\n",
    "import missingno as msno\n",
    "\n",
    "# Plot nullity matrix of airquality with frequency 'M'\n",
    "msno.matrix(airquality, freq='M')\n",
    "\n",
    "# Display nullity matrix\n",
    "display(\"/usr/local/share/datasets/matrix_frequency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import missingno as msno\n",
    "import missingno as msno\n",
    "\n",
    "# Plot the sliced nullity matrix of airquality with frequency 'M'\n",
    "msno.matrix(airquality.loc['May-1976': 'Jul-1976'], freq='M')\n",
    "\n",
    "# Display nullity matrix\n",
    "display(\"/usr/local/share/datasets/matrix_sliced.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import missingno\n",
    "import missingno as msno\n",
    "\n",
    "# Plot missingness heatmap of diabetes\n",
    "msno.heatmap(diabetes)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import missingno\n",
    "import missingno as msno\n",
    "\n",
    "# Plot missingness heatmap of diabetes\n",
    "msno.heatmap(diabetes)\n",
    "\n",
    "# Plot missingness dendrogram of diabetes\n",
    "msno.dendrogram(diabetes)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the missingness of a variable against another variable helps you determine any relationships between missing and non-missing values. This is very similar to how you found correlations of missingness between two columns in the previous exercises.\n",
    "\n",
    "In this exercise, you're going to write a function which fills in dummy values in the DataFrame and in the next exercise you are going to plot a scatter plot to analyze if there is any correlation of missingness against another column. The operations involve shifting and scaling the column range with a scaling factor.\n",
    "\n",
    "The function rand() has been imported for you from numpy.random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dummy_values(df, scaling_factor=0.075):\n",
    "  df_dummy = df.copy(deep=True)\n",
    "  for col_name in df_dummy:\n",
    "    col = df_dummy[col_name]\n",
    "    col_null = col.isnull()    \n",
    "    # Calculate number of missing values in column \n",
    "    num_nulls = col_null.sum()\n",
    "    # Calculate column range\n",
    "    col_range = col.max() - col.min()\n",
    "    # Scale the random values to scaling_factor times col_range\n",
    "    dummy_values = (rand(num_nulls) - 2) * scaling_factor * col_range + col.min()\n",
    "    col[col_null] = dummy_values\n",
    "  return df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill dummy values in diabetes_dummy\n",
    "diabetes_dummy = fill_dummy_values(diabetes)\n",
    "\n",
    "# Sum the nullity of Skin_Fold and BMI\n",
    "nullity = diabetes['Skin_Fold'].isnull()+diabetes['BMI'].isnull()\n",
    "\n",
    "# Create a scatter plot of Skin Fold and BMI \n",
    "diabetes_dummy.plot(x='Skin_Fold', y='BMI', kind='scatter', alpha=0.5,\n",
    "                    \n",
    "                    # Set color to nullity of BMI and Skin_Fold\n",
    "                    c=nullity, \n",
    "                    cmap='rainbow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missingness of diabetes prior to dropping missing values\n",
    "msno.matrix(diabetes)\n",
    "\n",
    "# Print the number of missing values in Glucose\n",
    "print(diabetes['Glucose'].isnull().sum())\n",
    "\n",
    "# Drop rows where 'Glucose' has a missing value\n",
    "diabetes.dropna(subset=['Glucose'], how='any', inplace=True)\n",
    "\n",
    "# Visualize the missingness of diabetes after dropping missing values\n",
    "msno.matrix(diabetes)\n",
    "\n",
    "display(\"/usr/local/share/datasets/glucose_dropped.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missingness in the data\n",
    "msno.matrix(diabetes)\n",
    "\n",
    "# Visualize the correlation of missingness between variables\n",
    "msno.heatmap(diabetes)\n",
    "\n",
    "# Show heatmap\n",
    "plt.show()\n",
    "\n",
    "# Drop rows where 'BMI' has a missing value\n",
    "diabetes.dropna(subset=['BMI'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of diabetes\n",
    "diabetes_median = diabetes.copy(deep=True)\n",
    "\n",
    "# Create median imputer object\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Impute median values in the DataFrame diabetes_median\n",
    "diabetes_median.iloc[:, :] = median_imputer.fit_transform(diabetes_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of diabetes\n",
    "diabetes_constant = diabetes.copy(deep=True)\n",
    "\n",
    "# Create median imputer object\n",
    "constant_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "# Impute missing values to 0 in diabetes_constant\n",
    "diabetes_constant.iloc[:, :] = constant_imputer.fit_transform(diabetes_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nrows and ncols to 2\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "nullity = diabetes.Serum_Insulin.isnull()+diabetes.Glucose.isnull()\n",
    "\n",
    "# Create a dictionary of imputations\n",
    "imputations = {'Mean Imputation': diabetes_mean, 'Median Imputation': diabetes_median, \n",
    "               'Most Frequent Imputation': diabetes_mode, 'Constant Imputation': diabetes_constant}\n",
    "\n",
    "# Loop over flattened axes and imputations\n",
    "for ax, df_key in zip(axes.flatten(), imputations):\n",
    "    # Select and also set the title for a DataFrame\n",
    "    imputations[df_key].plot(x='Serum_Insulin', y='Glucose', kind='scatter', \n",
    "                          alpha=0.5, c=nullity, cmap='rainbow', ax=ax, \n",
    "                          colorbar=False, title=df_key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prior to imputing missing values\n",
    "print(airquality[30:40])\n",
    "\n",
    "# Fill NaNs using backward fill\n",
    "airquality.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Print after imputing missing values\n",
    "print(airquality[30:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prior to interpolation\n",
    "print(airquality[30:40])\n",
    "\n",
    "# Interpolate the NaNs with nearest value\n",
    "airquality.interpolate(method='nearest', inplace=True)\n",
    "\n",
    "# Print after interpolation\n",
    "print(airquality[30:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualizaion of imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute airquality DataFrame with ffill method\n",
    "ffill_imputed = airquality.fillna(method='ffill')\n",
    "\n",
    "# Plot the imputed DataFrame ffill_imp in red dotted style \n",
    "ffill_imputed['Ozone'].plot(color='red', marker='o', linestyle='dotted', figsize=(30, 5))\n",
    "\n",
    "# Plot the airquality DataFrame with title\n",
    "airquality['Ozone'].plot(title='Ozone', marker='o', figsize=(30, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute airquality DataFrame with bfill method\n",
    "bfill_imputed = airquality.fillna(method='bfill')\n",
    "\n",
    "# Plot the imputed DataFrame bfill_imp in red dotted style \n",
    "bfill_imputed['Ozone'].plot(color='red', marker='o', linestyle='dotted', figsize=(30, 5))\n",
    "\n",
    "# Plot the airquality DataFrame with title\n",
    "airquality['Ozone'].plot(title='Ozone', marker='o', figsize=(30, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nrows to 3 and ncols to 1\n",
    "fig, axes = plt.subplots(3, 1, figsize=(30, 20))\n",
    "\n",
    "# Create a dictionary of interpolations\n",
    "interpolations = {'Linear Interpolation': linear, 'Quadratic Interpolation': quadratic, \n",
    "                  'Nearest Interpolation': nearest}\n",
    "\n",
    "# Loop over axes and interpolations\n",
    "for ax, df_key in zip(axes, interpolations):\n",
    "  # Select and also set the title for a DataFrame\n",
    "  interpolations[df_key].Ozone.plot(color='red', marker='o', \n",
    "                                 linestyle='dotted', ax=ax)\n",
    "  airquality.Ozone.plot(title=df_key + ' - Ozone', marker='o', ax=ax)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN from fancyimpute\n",
    "from fancyimpute import KNN\n",
    "\n",
    "# Copy diabetes to diabetes_knn_imputed\n",
    "diabetes_knn_imputed = diabetes.copy(deep=True)\n",
    "\n",
    "# Initialize KNN\n",
    "knn_imputer = KNN()\n",
    "\n",
    "# Impute using fit_tranform on diabetes_knn_imputed\n",
    "diabetes_knn_imputed.iloc[:, :] = knn_imputer.fit_transform(diabetes_knn_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IterativeImputer from fancyimpute\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "# Copy diabetes to diabetes_mice_imputed\n",
    "diabetes_mice_imputed = diabetes.copy(deep=True)\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "mice_imputer = IterativeImputer()\n",
    "\n",
    "# Impute using fit_tranform on diabetes\n",
    "diabetes_mice_imputed.iloc[:, :] = mice_imputer.fit_transform(diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ordinal encoder\n",
    "ambience_ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Select non-null values of ambience column in users\n",
    "ambience = users['ambience']\n",
    "ambience_not_null = ambience[ambience.notnull()]\n",
    "\n",
    "# Reshape ambience_not_null to shape (-1, 1)\n",
    "reshaped_vals = ambience_not_null.values.reshape(-1, 1)\n",
    "\n",
    "# Ordinally encode reshaped_vals\n",
    "encoded_vals = ambience_ord_enc.fit_transform(reshaped_vals)\n",
    "\n",
    "# Assign back encoded values to non-null values of ambience in users\n",
    "users.loc[ambience.notnull(), 'ambience'] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary ordinal_enc_dict\n",
    "ordinal_enc_dict = {}\n",
    "\n",
    "for col_name in users:\n",
    "    # Create Ordinal encoder for col\n",
    "    ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "    col = users[col_name]\n",
    "    \n",
    "    # Select non-null values of col\n",
    "    col_not_null = col[col.notnull()]\n",
    "    reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Store the values to non-null values of the column in users\n",
    "    users.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN imputer\n",
    "KNN_imputer = KNN()\n",
    "\n",
    "# Impute and round the users DataFrame\n",
    "users.iloc[:, :] = np.round(KNN_imputer.fit_transform(users))\n",
    "\n",
    "# Loop over the column names in users\n",
    "for col_name in users:\n",
    "    \n",
    "    # Reshape the data\n",
    "    reshaped = users[col_name].values.reshape(-1, 1)\n",
    "    \n",
    "    # Perform inverse transform of the ordinally encoded columns\n",
    "    users[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare imputes to remove bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant to X and set X & y values to fit linear model\n",
    "X = sm.add_constant(diabetes_cc.iloc[:, :-1])\n",
    "y = diabetes_cc['Class']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary of lm\n",
    "print('\\nSummary: ', lm.summary())\n",
    "\n",
    "# Print R squared score of lm\n",
    "print('\\nAdjusted R-squared score: ', lm.rsquared_adj)\n",
    "\n",
    "# Print the params of lm\n",
    "print('\\nCoefficcients:\\n', lm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squares = {'Mean Imputation': lm_mean.rsquared_adj, \n",
    "             'KNN Imputation': lm_KNN.rsquared_adj, \n",
    "             'MICE Imputation': lm_MICE.rsquared_adj}\n",
    "\n",
    "# Select best R-squared\n",
    "best_imputation = max(r_squares, key=r_squares.get)\n",
    "\n",
    "print(\"The best imputation technique is: \", best_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs of imputed DataFrames and the complete case\n",
    "diabetes_cc['Skin_Fold'].plot(kind='kde', c='red', linewidth=3)\n",
    "diabetes_mean_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_knn_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_mice_imputed['Skin_Fold'].plot(kind='kde')\n",
    "\n",
    "# Create labels for the four DataFrames\n",
    "labels = ['Baseline (Complete Case)', 'Mean Imputation', 'KNN Imputation', 'MICE Imputation']\n",
    "plt.legend(labels)\n",
    "\n",
    "# Set the x-label as Skin Fold\n",
    "plt.xlabel('Skin Fold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask how many features\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dimensions with no variance\n",
    "# Leave this list as is\n",
    "number_cols = ['HP', 'Attack', 'Defense']\n",
    "\n",
    "# Remove the feature without variance from this list\n",
    "non_number_cols = ['Name', 'Type']\n",
    "\n",
    "# Create a new dataframe by subselecting the chosen features\n",
    "df_selected = pokemon_df[number_cols + non_number_cols]\n",
    "\n",
    "# Prints the first 5 lines of the new dataframe\n",
    "print(df_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection vs Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one of the redundant features\n",
    "reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "sns.pairplot(reduced_df, hue='Gender')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the redundant feature\n",
    "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### t-SNE visualization of high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-numerical columns in the dataset\n",
    "non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Create a t-SNE model with learning rate 50\n",
    "m = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "print(tsne_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color the points by Army Branch\n",
    "sns.scatterplot(x=\"x\", y=\"y\", hue='Branch', data=df)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color the points by Gender\n",
    "sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "X = ansur_df.drop('Gender', axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy after reduction\n",
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "print(normalized_df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask on whether each feature less than 50% missing values.\n",
    "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
    "\n",
    "# Create a reduced dataset by applying the mask\n",
    "reduced_df = school_df.loc[:, mask]\n",
    "\n",
    "print(school_df.shape)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle \n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle \n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced_df dataframe has {} columns\".format(reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting features for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 2 features with the lowest model coefficients\n",
    "X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the feature with the highest coefficient\n",
    "X = diabetes_df[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fit the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Based feature selection\n",
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test set accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regularized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model \n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(alpha=0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### combining feature selectors  lassocv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA principle Componant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily XP\n",
    "2155\n",
    "Exercise\n",
    "Exercise\n",
    "Manual feature extraction I\n",
    "\n",
    "You want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID, product, quantity and revenue. The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it's more interesting to know the average price per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the price from the quantity sold and revenue\n",
    "sales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n",
    "\n",
    "# Drop the quantity and revenue features\n",
    "reduced_df = sales_df.drop(['revenue', 'quantity'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Legendary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let PCA select 90% of the variance\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=0.9))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "print('{} components selected'.format(len(pipe.steps[1][1].components_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### choosing number of components\n",
    "# Pipeline a scaler and pca selecting 10 components\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
