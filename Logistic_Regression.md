Logistic Regression

After this lesson you will be able to:

Describe the kind of problem Logistic regression can solve
Create a logistic regression model
Describe the elements of a Confusion Matrix
Topics/Highlights:

Logistic regression (notebook)
Glass identification dataset
e and natural log - what are they?
Exercise with Titanic data (notebook, data, data dictionary)
Confusion matrix (slides, notebook)
Homework:

Work through the code samples in the "Confusion matrix of Titanic predictions" section in the 09_confusion_matrix.ipynb notebook to see an example of changing a threshhold to get the desired behavior in the confusion matrix.
If you aren't yet comfortable with all of the confusion matrix terminology, watch Rahul Patwari's videos on Intuitive sensitivity and specificity (9 minutes) and The tradeoff between sensitivity and specificity (13 minutes).
Video/reading assignment on ROC curves and AUC
Video/reading assignment on cross-validation
Logistic Regression Resources:

To go deeper into logistic regression, read the first three sections of Chapter 4 of An Introduction to Statistical Learning, or watch the first three videos (30 minutes) from that chapter.
For a math-ier explanation of logistic regression, watch the first seven videos (71 minutes) from week 3 of Andrew Ng's machine learning course, or read the related lecture notes compiled by a student.
For more on interpreting logistic regression coefficients, read this excellent guide by UCLA's IDRE and these lecture notes from the University of New Mexico.
The scikit-learn documentation has a nice explanation of what it means for a predicted probability to be calibrated.
Supervised learning superstitions cheat sheet is a very nice comparison of four classifiers we cover in the course (logistic regression, decision trees, KNN, Naive Bayes) and one classifier we do not cover (Support Vector Machines).
Confusion Matrix Resources:

Kevin Markham's simple guide to confusion matrix terminology may be useful to you as a reference.
This blog post about Amazon Machine Learning contains a neat graphic showing how classification threshold affects different evaluation metrics.
This notebook (from another DAT course) explains how to calculate "expected value" from a confusion matrix by treating it as a cost-benefit matrix.
